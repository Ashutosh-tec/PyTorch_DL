{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 5) (5, 5) (5, 5)\n"
     ]
    }
   ],
   "source": [
    "#Initial set up\n",
    "\n",
    "# Define the one-hot encoding function\n",
    "def one_hot_encode(char, chars):\n",
    "    vocab_size = len(chars)\n",
    "    encoded = np.zeros(vocab_size)\n",
    "    encoded[chars.index(char)] = 1\n",
    "    return encoded.reshape(-1, 1)  #to change array in a vector\n",
    "\n",
    "# Define the vocabulary\n",
    "chars = ['l', 'e', 'a', 'r', 'n']\n",
    "\n",
    "# Test the function\n",
    "for i in chars:\n",
    "    print(one_hot_encode(i,chars).shape)\n",
    "\n",
    "    \n",
    "# Define the RNN parameters\n",
    "input_size = len(chars)\n",
    "hidden_size = 5  # Adjust as needed\n",
    "output_size = len(chars)\n",
    "learning_rate = 1\n",
    "\n",
    "parameters = np.load(\"parameters.npz\")\n",
    "U= parameters.get(\"U\")\n",
    "bu= parameters.get(\"bu\")\n",
    "W= parameters.get(\"W\")\n",
    "bw= parameters.get(\"bw\")\n",
    "V= parameters.get(\"V\")\n",
    "bv= parameters.get(\"bv\")\n",
    "np.savez(\"parameters\", U=U, bu=bu, W=W, bw=bw, V=V, bv=bv)\n",
    "\n",
    "print(U.shape, W.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)  # Subtracting the maximum value for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    s = softmax(x)\n",
    "    return np.diagflat(s) - np.outer(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  [[ 1.64131892]\n",
      " [-0.55871803]\n",
      " [-3.32402904]\n",
      " [-1.34598229]\n",
      " [-1.285274  ]]\n",
      "Loss at step 1 is: 2.400343465148997\n",
      "O:  [[ 2.35077265]\n",
      " [-1.19792664]\n",
      " [-1.75790813]\n",
      " [-0.84840265]\n",
      " [-1.07737616]]\n",
      "Loss at step 2 is: 6.620953965241554\n",
      "O:  [[ 0.87559356]\n",
      " [ 0.65151454]\n",
      " [-0.95385996]\n",
      " [-0.17767187]\n",
      " [ 0.32202598]]\n",
      "Loss at step 3 is: 8.733205261817995\n",
      "O:  [[ 0.97960047]\n",
      " [ 0.70541832]\n",
      " [ 1.02089437]\n",
      " [ 2.82080351]\n",
      " [-0.33059492]]\n",
      "Loss at step 4 is: 12.281579166788328\n",
      "Loss at last step is 12.281579166788328\n",
      "Total loss: 12.281579166788328\n",
      "0 [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "1 [[ 0.45952909]\n",
      " [-0.22995027]\n",
      " [-0.43286563]\n",
      " [-0.50967271]\n",
      " [ 0.89874415]]\n",
      "2 [[ 0.41653184]\n",
      " [ 0.99049789]\n",
      " [-0.85940849]\n",
      " [-0.99800625]\n",
      " [ 0.72569196]]\n",
      "3 [[ 0.99917404]\n",
      " [ 0.99856167]\n",
      " [ 0.04620508]\n",
      " [-0.99477806]\n",
      " [ 0.49342697]]\n",
      "4 [[ 0.99957558]\n",
      " [-0.15172561]\n",
      " [-0.89362858]\n",
      " [-0.9969986 ]\n",
      " [-0.92411435]]\n",
      "5 [[-0.66968424]\n",
      " [ 0.97496617]\n",
      " [ 0.63936819]\n",
      " [-0.93785853]\n",
      " [-0.80746262]]\n",
      "0 [[0.81847983]\n",
      " [0.0906868 ]\n",
      " [0.00570932]\n",
      " [0.04127048]\n",
      " [0.04385356]]\n",
      "1 [[0.8941071 ]\n",
      " [0.02571633]\n",
      " [0.01468967]\n",
      " [0.03647585]\n",
      " [0.02901105]]\n",
      "2 [[0.34680734]\n",
      " [0.27718645]\n",
      " [0.05566301]\n",
      " [0.12096533]\n",
      " [0.19937786]]\n",
      "3 [[0.10665259]\n",
      " [0.08107662]\n",
      " [0.11114889]\n",
      " [0.67235051]\n",
      " [0.02877139]]\n",
      "4 [[0.03782231]\n",
      " [0.03585545]\n",
      " [0.27509607]\n",
      " [0.09915536]\n",
      " [0.55207081]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = chars\n",
    "total_loss = 0\n",
    "true_label_s5 = np.zeros((output_size, 1))\n",
    "\n",
    "# Initialize dictionaries to store values at each time step\n",
    "x = {}\n",
    "s = {}\n",
    "\n",
    "sprev = np.array([0,0,0,0,0]).reshape(-1,1)\n",
    "s[0]=sprev\n",
    "true_y = {}\n",
    "y_hat = {}\n",
    "\n",
    "# Perform forward propagation and compute loss for each time step\n",
    "for i in range(len(input_sequence)):  # Iterate through each character\n",
    "    # Compute input at current time step\n",
    "    x[i] = one_hot_encode(input_sequence[i], chars)\n",
    "    \n",
    "    # Compute hidden state\n",
    "    current_s = np.tanh(np.dot(U, x[i]) + np.dot(W, sprev))\n",
    "    s[i+1] = current_s\n",
    "    \n",
    "    # Compute output probabilities and true label\n",
    "    if i < len(input_sequence) - 1:\n",
    "        # Compute output probabilities for all but the last character\n",
    "        O = np.dot(V, current_s)\n",
    "        print(\"O: \", O)\n",
    "        current_y_hat = softmax(O)\n",
    "        y_hat[i] = current_y_hat\n",
    "        \n",
    "        # True label for the next character in the sequence\n",
    "        true_y[i] = one_hot_encode(input_sequence[i + 1], chars)\n",
    "        \n",
    "        # Compute loss at this time step\n",
    "        loss = -np.sum(true_y[i] * np.log(current_y_hat))\n",
    "        \n",
    "        # Accumulate total loss\n",
    "        total_loss += loss\n",
    "        print(\"Loss at step\", i + 1, \"is:\", total_loss)\n",
    "    \n",
    "    else:\n",
    "        # For the last character in the sequence\n",
    "        # Compute output probabilities for the last character\n",
    "        O = np.dot(V, current_s)\n",
    "        current_y_hat = softmax(O)\n",
    "        y_hat[i] = current_y_hat\n",
    "        \n",
    "        # True label for the last character is given as true_label_s5\n",
    "        true_y[i] = true_label_s5\n",
    "        \n",
    "        # Compute loss at the last step\n",
    "        loss = -np.sum(true_label_s5 * np.log(current_y_hat))\n",
    "        total_loss += loss\n",
    "        print(\"Loss at last step is\", total_loss)\n",
    "\n",
    "    # Update previous hidden state for next time step\n",
    "    sprev = current_s\n",
    "\n",
    "\n",
    "# Print total loss\n",
    "print(\"Total loss:\", total_loss)\n",
    "\n",
    "# Now you have dictionaries x, s, true_y, and y_hat containing values for each time step\n",
    "for i in s:\n",
    "    print(i, s[i])\n",
    "for i in y_hat:\n",
    "    print(i, y_hat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1st_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
