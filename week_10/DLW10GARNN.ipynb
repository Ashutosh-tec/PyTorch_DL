{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2892fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcbc3fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "(5, 5) (5, 5) (5, 5)\n"
     ]
    }
   ],
   "source": [
    "#Initial set up\n",
    "\n",
    "# Define the one-hot encoding function\n",
    "def one_hot_encode(char, chars):\n",
    "    vocab_size = len(chars)\n",
    "    encoded = np.zeros(vocab_size)\n",
    "    encoded[chars.index(char)] = 1\n",
    "    return encoded.reshape(-1, 1)  #to change array in a vector\n",
    "\n",
    "# Define the vocabulary\n",
    "chars = ['l', 'e', 'a', 'r', 'n']\n",
    "\n",
    "# Test the function\n",
    "for i in chars:\n",
    "    print(one_hot_encode(i,chars).shape)\n",
    "\n",
    "    \n",
    "# Define the RNN parameters\n",
    "input_size = len(chars)\n",
    "hidden_size = 5  # Adjust as needed\n",
    "output_size = len(chars)\n",
    "learning_rate = 1\n",
    "\n",
    "parameters = np.load(r\"C:\\Users\\Dell\\Documents\\IITM\\IITM7\\DL\\parameters.npz\")\n",
    "U= parameters.get(\"U\")\n",
    "bu= parameters.get(\"bu\")\n",
    "W= parameters.get(\"W\")\n",
    "bw= parameters.get(\"bw\")\n",
    "V= parameters.get(\"V\")\n",
    "bv= parameters.get(\"bv\")\n",
    "np.savez(\"parameters\", U=U, bu=bu, W=W, bw=bw, V=V, bv=bv)\n",
    "\n",
    "print(U.shape, W.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd845a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Define derivative of tanh function\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    s = softmax(x)\n",
    "    return np.diagflat(s) - np.outer(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4db6ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FirstPass and first step (to understand)\\n\\nsprev = np.zeros((hidden_size, 1))  # Initial hidden state\\n\\n# Compute the input at the current time step\\nx = one_hot_encode(\\'l\\', chars)  # Reshape to a column vector\\ninput_contribution = np.dot(U, x)\\n\\n# Compute the contribution from the previous hidden state\\nhidden_contribution = np.dot(W, sprev)\\n\\n# Add input and hidden contributions, and bias term to obtain the current hidden state\\ns = input_contribution + hidden_contribution \\n\\n# Apply tanh activation function to the current hidden state\\ns = np.tanh(s)\\n\\n# Print the shapes for understanding\\nprint(\"Shape of input contribution:\", input_contribution.shape)\\nprint(\"Shape of hidden contribution:\", hidden_contribution.shape)\\nprint(\"Shape of current hidden state:\", s.shape)\\nprint(s)\\nprint(s.shape)\\nprint(np.sum(s))\\n\\nO=np.dot(V,s)\\nprint(O, O.shape, np.sum(O))\\n\\n# Compute the softmax probabilities\\nsoftmax_probs = softmax(O)\\n\\n# Print the softmax probabilities\\nprint(\"Softmax probabilities:\", softmax_probs)\\n\\n# Define the true probability distribution (one-hot encoded for \\'e\\')\\ntrue_probs = one_hot_encode(\\'e\\', chars)\\n\\n# Compute the cross-entropy loss\\nloss = -np.sum(true_probs * np.log(softmax_probs))\\n\\n# Print the loss\\nprint(\"Cross-entropy loss:\", loss)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#FirstPass and first step (to understand)\n",
    "\n",
    "sprev = np.zeros((hidden_size, 1))  # Initial hidden state\n",
    "\n",
    "# Compute the input at the current time step\n",
    "x = one_hot_encode('l', chars)  # Reshape to a column vector\n",
    "input_contribution = np.dot(U, x)\n",
    "\n",
    "# Compute the contribution from the previous hidden state\n",
    "hidden_contribution = np.dot(W, sprev)\n",
    "\n",
    "# Add input and hidden contributions, and bias term to obtain the current hidden state\n",
    "s = input_contribution + hidden_contribution \n",
    "\n",
    "# Apply tanh activation function to the current hidden state\n",
    "s = np.tanh(s)\n",
    "\n",
    "# Print the shapes for understanding\n",
    "print(\"Shape of input contribution:\", input_contribution.shape)\n",
    "print(\"Shape of hidden contribution:\", hidden_contribution.shape)\n",
    "print(\"Shape of current hidden state:\", s.shape)\n",
    "print(s)\n",
    "print(s.shape)\n",
    "print(np.sum(s))\n",
    "\n",
    "O=np.dot(V,s)\n",
    "print(O, O.shape, np.sum(O))\n",
    "\n",
    "# Compute the softmax probabilities\n",
    "softmax_probs = softmax(O)\n",
    "\n",
    "# Print the softmax probabilities\n",
    "print(\"Softmax probabilities:\", softmax_probs)\n",
    "\n",
    "# Define the true probability distribution (one-hot encoded for 'e')\n",
    "true_probs = one_hot_encode('e', chars)\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "loss = -np.sum(true_probs * np.log(softmax_probs))\n",
    "\n",
    "# Print the loss\n",
    "print(\"Cross-entropy loss:\", loss)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0f156d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1 is: 2.400343465148997\n",
      "Loss at step 2 is: 6.620953965241555\n",
      "Loss at step 3 is: 8.733205261817997\n",
      "Loss at step 4 is: 12.28157916678833\n",
      "Loss at last step is 12.28157916678833\n",
      "Total loss: 12.28157916678833\n",
      "0 [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "1 [[ 0.45952909]\n",
      " [-0.22995027]\n",
      " [-0.43286563]\n",
      " [-0.50967271]\n",
      " [ 0.89874415]]\n",
      "2 [[ 0.41653184]\n",
      " [ 0.99049789]\n",
      " [-0.85940849]\n",
      " [-0.99800625]\n",
      " [ 0.72569196]]\n",
      "3 [[ 0.99917404]\n",
      " [ 0.99856167]\n",
      " [ 0.04620508]\n",
      " [-0.99477806]\n",
      " [ 0.49342697]]\n",
      "4 [[ 0.99957558]\n",
      " [-0.15172561]\n",
      " [-0.89362858]\n",
      " [-0.9969986 ]\n",
      " [-0.92411435]]\n",
      "5 [[-0.66968424]\n",
      " [ 0.97496617]\n",
      " [ 0.63936819]\n",
      " [-0.93785853]\n",
      " [-0.80746262]]\n",
      "0 [[0.81847983]\n",
      " [0.0906868 ]\n",
      " [0.00570932]\n",
      " [0.04127048]\n",
      " [0.04385356]]\n",
      "1 [[0.8941071 ]\n",
      " [0.02571633]\n",
      " [0.01468967]\n",
      " [0.03647585]\n",
      " [0.02901105]]\n",
      "2 [[0.34680734]\n",
      " [0.27718645]\n",
      " [0.05566301]\n",
      " [0.12096533]\n",
      " [0.19937786]]\n",
      "3 [[0.10665259]\n",
      " [0.08107662]\n",
      " [0.11114889]\n",
      " [0.67235051]\n",
      " [0.02877139]]\n",
      "4 [[0.03782231]\n",
      " [0.03585545]\n",
      " [0.27509607]\n",
      " [0.09915536]\n",
      " [0.55207081]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = chars\n",
    "total_loss = 0\n",
    "true_label_s5 = np.zeros((output_size, 1))\n",
    "\n",
    "# Initialize dictionaries to store values at each time step\n",
    "x = {}\n",
    "s = {}\n",
    "loss = {}\n",
    "\n",
    "sprev = np.array([0,0,0,0,0]).reshape(-1,1)\n",
    "s[0]=sprev\n",
    "true_y = {}\n",
    "y_hat = {}\n",
    "\n",
    "# Perform forward propagation and compute loss for each time step\n",
    "for i in range(len(input_sequence)):  # Iterate through each character\n",
    "    # Compute input at current time step\n",
    "    x[i] = one_hot_encode(input_sequence[i], chars)\n",
    "    \n",
    "    # Compute hidden state\n",
    "    current_s = np.tanh(np.dot(U, x[i]) + np.dot(W, sprev))\n",
    "    s[i+1] = current_s\n",
    "    \n",
    "    # Compute output probabilities and true label\n",
    "    if i < len(input_sequence) - 1:\n",
    "        # Compute output probabilities for all but the last character\n",
    "        O = np.dot(V, current_s)\n",
    "        current_y_hat = softmax(O)\n",
    "        y_hat[i] = current_y_hat\n",
    "        \n",
    "        # True label for the next character in the sequence\n",
    "        true_y[i] = one_hot_encode(input_sequence[i + 1], chars)\n",
    "        \n",
    "        # Compute loss at this time step\n",
    "        loss1 = -np.sum(true_y[i] * np.log(current_y_hat))\n",
    "        loss[i]=loss1\n",
    "        # Accumulate total loss\n",
    "        total_loss += loss1\n",
    "        print(\"Loss at step\", i + 1, \"is:\", total_loss)\n",
    "    \n",
    "    else:\n",
    "        # For the last character in the sequence\n",
    "        # Compute output probabilities for the last character\n",
    "        O = np.dot(V, current_s)\n",
    "        current_y_hat = softmax(O)\n",
    "        y_hat[i] = current_y_hat\n",
    "        \n",
    "        # True label for the last character is given as true_label_s5\n",
    "        true_y[i] = true_label_s5\n",
    "        \n",
    "        # Compute loss at the last step\n",
    "        loss1 = -np.sum(true_label_s5 * np.log(current_y_hat))\n",
    "        loss[i]=loss1\n",
    "        total_loss += loss1\n",
    "        print(\"Loss at last step is\", total_loss)\n",
    "\n",
    "    # Update previous hidden state for next time step\n",
    "    sprev = current_s\n",
    "\n",
    "\n",
    "# Print total loss\n",
    "print(\"Total loss:\", total_loss)\n",
    "\n",
    "# Now you have dictionaries x, s, true_y, and y_hat containing values for each time step\n",
    "for i in s:\n",
    "    print(i, s[i])\n",
    "for i in y_hat:\n",
    "    print(i, y_hat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7fb46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.400343465148997\n",
      "1 4.220610500092558\n",
      "2 2.112251296576441\n",
      "3 3.548373904970334\n",
      "4 -0.0\n",
      "0 [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "1 [[ 0.45952909]\n",
      " [-0.22995027]\n",
      " [-0.43286563]\n",
      " [-0.50967271]\n",
      " [ 0.89874415]]\n",
      "2 [[ 0.41653184]\n",
      " [ 0.99049789]\n",
      " [-0.85940849]\n",
      " [-0.99800625]\n",
      " [ 0.72569196]]\n",
      "3 [[ 0.99917404]\n",
      " [ 0.99856167]\n",
      " [ 0.04620508]\n",
      " [-0.99477806]\n",
      " [ 0.49342697]]\n",
      "4 [[ 0.99957558]\n",
      " [-0.15172561]\n",
      " [-0.89362858]\n",
      " [-0.9969986 ]\n",
      " [-0.92411435]]\n",
      "5 [[-0.66968424]\n",
      " [ 0.97496617]\n",
      " [ 0.63936819]\n",
      " [-0.93785853]\n",
      " [-0.80746262]]\n",
      "0 [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "1 [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "2 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "3 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "4 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0 [[0.81847983]\n",
      " [0.0906868 ]\n",
      " [0.00570932]\n",
      " [0.04127048]\n",
      " [0.04385356]]\n",
      "1 [[0.8941071 ]\n",
      " [0.02571633]\n",
      " [0.01468967]\n",
      " [0.03647585]\n",
      " [0.02901105]]\n",
      "2 [[0.34680734]\n",
      " [0.27718645]\n",
      " [0.05566301]\n",
      " [0.12096533]\n",
      " [0.19937786]]\n",
      "3 [[0.10665259]\n",
      " [0.08107662]\n",
      " [0.11114889]\n",
      " [0.67235051]\n",
      " [0.02877139]]\n",
      "4 [[0.03782231]\n",
      " [0.03585545]\n",
      " [0.27509607]\n",
      " [0.09915536]\n",
      " [0.55207081]]\n",
      "0 [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "1 [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "2 [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "3 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "4 [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in loss:\n",
    "    print(i, loss[i])\n",
    "    \n",
    "for i in s:\n",
    "    print(i, s[i])\n",
    "    \n",
    "for i in true_y:\n",
    "    print(i, true_y[i])\n",
    "    \n",
    "for i in y_hat:\n",
    "    print(i, y_hat[i])\n",
    "    \n",
    "for i in x:\n",
    "    print(i, x[i])\n",
    "    \n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fec5ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.46439149e-02  9.06365905e-03 -9.75266134e-03 -9.62839780e-03\n",
      "  -1.15541539e-01]\n",
      " [ 2.15911541e-01  2.08395239e-01 -8.00368924e-04 -2.13087787e-01\n",
      "   3.63776824e-02]\n",
      " [ 5.92922012e-02  2.19958265e-02 -4.52899269e-01 -1.96689787e-01\n",
      "  -7.92012029e-01]\n",
      " [ 8.52400553e-03 -7.64296584e-03 -2.84259913e-03 -3.44889969e-03\n",
      "  -2.57198334e-02]\n",
      " [ 6.88004285e-01  6.27930474e-01 -1.56018113e+00 -1.54669826e+00\n",
      "   3.00403427e+00]] -0.04277242516305568\n"
     ]
    }
   ],
   "source": [
    "# Back Propagation\n",
    "# Initialize gradients\n",
    "grad_U = np.zeros_like(U)\n",
    "grad_V = np.zeros_like(V)\n",
    "grad_W = np.zeros_like(W)\n",
    "grad_b = np.zeros_like(bu)\n",
    "grad_c = np.zeros_like(bv)\n",
    "\n",
    "loss=0\n",
    "# Initialize d_h_next\n",
    "d_s_next = np.zeros((hidden_size, 1))\n",
    "\n",
    "for t in reversed(range(1, len(s))):  # Iterate from the last time step backwards\n",
    "    #print(\"step\", t)   \n",
    "    '''In this network there are 5 hidden layers\n",
    "    At s(5) feed from s(4) and x(4), and pred_y=y_hat[4], and true_y=y[4]'''\n",
    "    # At s(5), there is no loss as true_y[4] =[0 0 0 0 0]\n",
    "    # Compute cross-entropy loss (as a scalar)\n",
    "    #print(y_hat[t-1], true_y[t-1])\n",
    "    loss += -np.sum(np.log(y_hat[t-1]) * true_y[t-1])\n",
    "    #print(loss)\n",
    "    # Backpropagate into output (derivative of cross-entropy)\n",
    "    d_o = y_hat[t-1] - true_y[t-1]\n",
    "    \n",
    "    # Backpropagate into V\n",
    "    grad_V += np.dot(d_o, s[t].T)\n",
    "    #print(grad_V)\n",
    "    # Backpropagate into s\n",
    "    d_s = np.dot(V.T, d_o) + d_s_next  # Sum of implicit and explicit loss)\n",
    "    #print(d_s)                                   #for last layer d_s_next is all 0s\n",
    "    # Backpropagate through non-linearity\n",
    "    d_h =  d_s * (1 - s[t]**2)\n",
    "    #print(d_h)\n",
    "    grad_U += np.dot(d_h, x[t-1].T)\n",
    "\n",
    "    grad_W += np.dot(d_h, s[t-1].T)\n",
    "    #print(\"Loss gradient wrt W at present layer is\", grad_W)\n",
    "    # Update d_s_next for the next iteration\n",
    "    d_s_next = np.dot(W.T, d_h)\n",
    "    #print(\"Loss Flwoing back from this layer is\", d_s_next)\n",
    "print(grad_W, grad_W.sum())\n",
    "    # Update weights and biases using gradients\n",
    "U -= learning_rate * grad_U\n",
    "V -= learning_rate * grad_V\n",
    "W -= learning_rate * grad_W\n",
    "\n",
    "#print(learning_rate)\n",
    "#print(W, W.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "776b98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]] [[0.01765745]\n",
      " [0.08256517]\n",
      " [0.03294536]\n",
      " [0.06506616]\n",
      " [0.80176586]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]] [[ 0.99965191]\n",
      " [-0.93944717]\n",
      " [-0.81241202]\n",
      " [-0.99048735]\n",
      " [-0.99988941]]\n",
      "Predicted sequence: enlrn\n"
     ]
    }
   ],
   "source": [
    "#Prediction now,  ``Auto-regressive mode'' (that is, \n",
    "#the input to the next time step is the output from the previous time step)\n",
    "# Initialize the initial hidden state\n",
    "sprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Initialize a list to store the predicted characters\n",
    "predicted_sequence = []\n",
    "\n",
    "# Iterate through each time step in the sequence\n",
    "x_current = one_hot_encode(input_sequence[0], chars)\n",
    "for i in range(len(input_sequence)):\n",
    "    # Compute input at current time step\n",
    "    \n",
    "    \n",
    "    # Compute hidden state for the current time step using the updated weights\n",
    "    current_s = np.tanh(np.dot(U, x_current) + np.dot(W, sprev))\n",
    "    \n",
    "    # Compute output probabilities for the current time step using the updated weights\n",
    "    O = np.dot(V, current_s)\n",
    "    current_y_hat = softmax(O)\n",
    "    if i == 4:\n",
    "        print(x_current , current_y_hat) \n",
    "    # Get the index of the predicted character (argmax of output probabilities)\n",
    "    predicted_index = np.argmax(current_y_hat)\n",
    "    \n",
    "    # Get the predicted character from the vocabulary\n",
    "    predicted_char = chars[predicted_index]\n",
    "    \n",
    "    # Append the predicted character to the list of predicted sequence\n",
    "    predicted_sequence.append(predicted_char)\n",
    "    \n",
    "    # Update the hidden state for the next time step\n",
    "    \n",
    "    x_current = np.int8(current_y_hat >=current_y_hat.max() )\n",
    "    sprev = current_s\n",
    "    if i == 4:\n",
    "        print(x_current , current_s)\n",
    "# Join the predicted characters to form the predicted sequence\n",
    "predicted_sequence_str = ''.join(predicted_sequence)\n",
    "\n",
    "# Print the predicted sequence\n",
    "print(\"Predicted sequence:\", predicted_sequence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886cd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
