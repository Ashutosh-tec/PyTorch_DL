{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will be implementing a GPT model and train it using CLM objective.\n",
        " * If you get stuck at something or need more clarrifications, you may refer to : https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
        "\n",
        " * We will be using ReLU activation function instead of GELU.\n",
        "\n",
        " * As usual, let us install the required libraries\n",
        "\n",
        " * **Note** that if you are not getting the exact loss values as mentioned in this notebook, that is absolutely fine. Just see whether your implementation overfits the given toy-and-tiny paragraph!"
      ],
      "metadata": {
        "id": "lYT5NXV-2eXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n"
      ],
      "metadata": {
        "id": "hdKopIVVtzKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata==0.6.0 # to be compatible with torch 2.0\n",
        "!pip install portalocker==2.0.0"
      ],
      "metadata": {
        "id": "HF0ap-B02CW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b196fe-6cff-487e-e759-7df179b3d2d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (919 bytes)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.32.3)\n",
            "Collecting torch==2.0.0 (from torchdata==0.6.0)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.45.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 triton-2.0.0\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* See [here](https://github.com/pytorch/text) for compatability"
      ],
      "metadata": {
        "id": "37SOAt7OE8pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.15.1"
      ],
      "metadata": {
        "id": "amMg7VeGEnui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd45831-af7f-4055-f12b-542978464084"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.45.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
            "Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "OFnTb-YGtudi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NCbW1AUlZsoO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "IjjPjidG6kr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd375b67-0da3-4894-eb89-2d4d9e850796"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset for LM modeling\n",
        "\n",
        " * We use a simple tokenizer and put"
      ],
      "metadata": {
        "id": "4wTjUVQRaoqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10"
      ],
      "metadata": {
        "id": "-aWB84SAN00b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.text = text\n",
        "    self.word_tokenizer = get_tokenizer(tokenizer=\"basic_english\",language='en')\n",
        "    self.vocab_size = None\n",
        "\n",
        "  def get_tokens(self):\n",
        "    for sentence in self.text.strip().split('\\n'):\n",
        "      yield self.word_tokenizer(sentence)\n",
        "\n",
        "  def build_vocab(self):\n",
        "    v = build_vocab_from_iterator(self.get_tokens(),\n",
        "                                  min_freq=1,specials=['<unk>','<start>','<end>'])\n",
        "    v.set_default_index(v['<unk>']) # index of OOV\n",
        "    self.vocab_size = len(v)\n",
        "    self.vocab = v  # Store the vocab in the object\n",
        "\n",
        "    return v\n",
        "\n",
        "  def token_ids(self):\n",
        "    v = self.build_vocab()\n",
        "    vt = VocabTransform(v)\n",
        "    num_tokens = len(self.word_tokenizer(self.text))\n",
        "    max_seq_len = np.ceil(num_tokens/batch_size)\n",
        "    data = torch.zeros(size=(1,num_tokens))\n",
        "    data = vt(self.word_tokenizer(self.text))\n",
        "    data = torch.tensor(data,dtype=torch.int64)\n",
        "    return data.reshape(batch_size,torch.tensor(max_seq_len,dtype=torch.int64))\n",
        "\n"
      ],
      "metadata": {
        "id": "EBzIVx8u5VQU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Best known for the invention of Error Correcting Codes, he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines.\n",
        "Reflecting on the significant benefits I received from Hamming, I decided to develop a tribute to his legacy. There has not been a previous biography of Hamming, and the few articles about him restate known facts and assumptions and leave us with open questions.\n",
        "One thought drove me as I developed this legacy project: An individual's legacy is more than a list of their attempts and accomplishments. Their tribute should also reveal the succeeding generations they inspired and enabled and what each attempted and achieved.\n",
        "This book is a unique genre containing my version of a biography that intertwines the story \"of a life\" and a multi-player memoir with particular events and turning points recalled by those, including me, who he inspired and enabled.\n",
        "Five years of research uncovered the people, places, opportunities, events, and influences that shaped Hamming. I discovered unpublished information, stories, photographs, videos, and personal remembrances to chronicle his life, which helped me put Hamming's\n",
        "legacy in the context I wanted.The result demonstrates many exceptional qualities, including his noble pursuit of excellence and helping others. Hamming paid attention to the details, his writings continue to influence, and his guidance is a timeless gift to the world.\n",
        "This biography is part of \"\"\""
      ],
      "metadata": {
        "id": "FE1L07Z-AoNz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tk = Tokenizer(text)"
      ],
      "metadata": {
        "id": "-OD9gBdaBHs1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_raw = Tk.token_ids()\n",
        "print(x_raw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTz7eGsSFK24",
        "outputId": "d21f20cf-ec7e-4b80-e533-44cbae00e9a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let us display the first 10 tokens of the vocabulary\n",
        "v = Tk.build_vocab()\n",
        "pprint(v.vocab.get_itos()[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToL6j7ECOEIC",
        "outputId": "3e9bdd61-f8ff-408a-c7e2-55e82c1def4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<start>', '<end>', ',', 'and', '.', 'the', 'a', 'of', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create the input_ids and Labels from the raw input sequence"
      ],
      "metadata": {
        "id": "Gs00A32ieNJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs,raw_seq_len = x_raw.shape\n",
        "x = torch.empty(size=(bs,raw_seq_len+2),dtype=torch.int64)\n",
        "x[:,1:-1] =x_raw\n",
        "\n",
        "# insert the index of special tokens\n",
        "x[:,0] = torch.full(size=(1,batch_size),fill_value=v.vocab.get_stoi()['<start>'])\n",
        "x[:,-1] = torch.full(size=(1,batch_size),fill_value=v.vocab.get_stoi()['<end>'])\n",
        "\n",
        "#Quickly check implem\n",
        "v = Tk.build_vocab()\n",
        "words = []\n",
        "for idx in x[0,:]:\n",
        "  words.append(v.vocab.get_itos()[idx.item()])\n",
        "print(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dCg4PYfaXhP",
        "outputId": "34375ee1-b07f-465c-a240-cb1db32516c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> best known for the invention of error correcting codes , he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels are just the input_ids shifted by right\n",
        "bs,seq_len = x.shape\n",
        "y = torch.empty(size=(bs,seq_len),dtype=torch.int64)\n",
        "y[:,0:-1] = copy.deepcopy(x[:,1:])\n",
        "\n",
        "#ignore the index of padded tokens while computing loss\n",
        "y[:,-1] = torch.full(size=(1,batch_size),fill_value=-100)"
      ],
      "metadata": {
        "id": "A-Y0R3FSfEex"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "z8-18UlZ2ES7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = Tk.vocab_size\n",
        "seq_len = x.shape[1]\n",
        "embed_dim = 32\n",
        "dmodel = embed_dim\n",
        "dq = torch.tensor(4)\n",
        "dk = torch.tensor(4)\n",
        "dv = torch.tensor(4)\n",
        "heads = torch.tensor(8)\n",
        "d_ff = 4*dmodel"
      ],
      "metadata": {
        "id": "2kgKn1Emakah"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Define all the sub-layers (mhma,ffn) in the transformer blocks\n",
        "* Seed for $W_Q,W_K,W_V,W_O$, 43, 44 and 45, 46, respectively\n",
        "* Seed for ffn $W_1,W_2$,  47 and 48. There are no biases\n",
        "* Seed for output layer 49"
      ],
      "metadata": {
        "id": "oyQlw_Q9OLFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(batch_size, head, seq1, seq2):\n",
        "  # Create a tensor of size (dim, dim) filled with negative infinity\n",
        "  tensor = torch.full((batch_size, head, seq1, seq2), float('-inf'))\n",
        "  # print(tensor.shape)\n",
        "  # Fill the upper triangular part (including the diagonal) with zeros\n",
        "  tensor = torch.triu(tensor, diagonal=0)\n",
        "  # print(tensor.shape)\n",
        "\n",
        "  # mask = torch.concat([tensor[i].fill_diagonal_(0) for i in range(batch_size)])\n",
        "  # Fill the diagonal with zeros for each batch slice in-place\n",
        "  for i in range(batch_size):\n",
        "    for j in range(head):\n",
        "      tensor[i][j].fill_diagonal_(0)\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "52Ofw1ZY0a90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MHMA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads,mask=None):\n",
        "    super(MHMA,self).__init__()\n",
        "    # your code goes here\n",
        "    self.dq = dq\n",
        "    self.dk = dk\n",
        "    self.dv = dv\n",
        "    self.heads = heads\n",
        "    self.mask = 1\n",
        "    self.W_Q = torch.randn(size=(heads, dmodel, dq),generator=torch.random.manual_seed(43))\n",
        "    self.W_K = torch.randn(size=(heads, dmodel, dq),generator=torch.random.manual_seed(44))\n",
        "    self.W_V = torch.randn(size=(heads, dmodel, dq),generator=torch.random.manual_seed(45))\n",
        "    self.W_O = torch.randn(size=(dmodel, dmodel),generator=torch.random.manual_seed(46))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # implement forward method\n",
        "\n",
        "    # x ->  batch_size, seq_len, dmodel\n",
        "    # Wq -> heads, dmodel, dq\n",
        "\n",
        "    Q = torch.einsum('bsd,hdq->bhsq',x, self.W_Q)\n",
        "    K = torch.einsum('bsd,hdq->bhsq',x, self.W_K)\n",
        "    V = torch.einsum('bsd,hdq->bhsq',x, self.W_V)\n",
        "\n",
        "    temp_att = torch.einsum('bhsq,bhtq->bhst', Q, K)\n",
        "    if self.mask is None:\n",
        "      self.mask = create_mask(batch_size, self.heads, temp_att.shape[2], temp_att.shape[3])\n",
        "    temp_att = temp_att + self.mask\n",
        "    z = torch.softmax(temp_att, dim = -1)\n",
        "    # z -> batch_size, head, seq_len (in Q), seq_len (in K)\n",
        "    out = torch.einsum('bhst,bhtq->bhsq', z, V)\n",
        "    # out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.heads * self.dv * 2)\n",
        "    # out = torch.einsum('bsd,od->bso', out, self.W_O)  #-> batch_size, seq_len, dmodel\n",
        "    # Combine attention heads\n",
        "    out = out.transpose(1, 2).contiguous().view(x.shape[0], x.shape[1], -1)\n",
        "    out = out @ self.W_O\n",
        "    return out"
      ],
      "metadata": {
        "id": "OcIN3dlGt8o6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, embed_dim, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        # Seeds for reproducibility\n",
        "        torch.manual_seed(47)\n",
        "        self.W1 = nn.Parameter(torch.randn(embed_dim, d_ff))\n",
        "\n",
        "        torch.manual_seed(48)\n",
        "        self.W2 = nn.Parameter(torch.randn(d_ff, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply ReLU activation instead of GELU\n",
        "        x = F.relu(x @ self.W1)\n",
        "        x = x @ self.W2\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8STxDeka8rVO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(PredictionHead, self).__init__()\n",
        "\n",
        "        # Seed for reproducibility\n",
        "        torch.manual_seed(49)\n",
        "        self.W_out = nn.Parameter(torch.randn(dmodel, vocab_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W_out\n"
      ],
      "metadata": {
        "id": "LOx4NTo-8rYR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, embed_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.positional_encoding = torch.zeros(seq_len, embed_dim)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "\n",
        "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.positional_encoding = self.positional_encoding.unsqueeze(0)  # Shape: (1, seq_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.positional_encoding[:, :x.size(1), :]\n"
      ],
      "metadata": {
        "id": "hPfd_uTw8rbH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads, mask=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mhma = MHMA(dmodel, heads, dq, dk, dv)\n",
        "        self.layer_norm_1 = nn.LayerNorm(dmodel)\n",
        "        self.layer_norm_2 = nn.LayerNorm(dmodel)\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "\n",
        "    def forward(self, dec_rep):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dec_rep: Tensor of shape (batch_size, seq_len, dmodel)\n",
        "\n",
        "        Returns:\n",
        "            out: Tensor of shape (batch_size, seq_len, dmodel)\n",
        "        \"\"\"\n",
        "        # Multi-Head Attention + Residual + LayerNorm\n",
        "        attention_out = self.mhma(dec_rep)\n",
        "        residual_1 = dec_rep + attention_out\n",
        "        norm_1 = self.layer_norm_1(residual_1)\n",
        "\n",
        "        # Feed-Forward Network + Residual + LayerNorm\n",
        "        ffn_out = self.ffn(norm_1)\n",
        "        residual_2 = norm_1 + ffn_out\n",
        "        out = self.layer_norm_2(residual_2)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-Z_eWhYUz4rQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embed, self).__init__()\n",
        "\n",
        "        # Seed for reproducibility\n",
        "        torch.manual_seed(70)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pe = PositionalEncoding(seq_len=seq_len, embed_dim=embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            out: Tensor of shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Apply embedding and positional encoding\n",
        "        out = self.pe(self.embed(x))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "MuaoeZtd0htC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask,num_layers=1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embed_lookup = Embed(vocab_size,embed_dim)\n",
        "    self.dec_layers = nn.ModuleList(copy.deepcopy(DecoderLayer(dmodel,dq,dk,dv,d_ff,heads,mask)) for i in range(num_layers))\n",
        "    self.predict = PredictionHead(dmodel,vocab_size)\n",
        "\n",
        "  def forward(self,input_ids):\n",
        "    out = self.embed_lookup(input_ids)\n",
        "    for dec_layer in self.dec_layers:\n",
        "      out = dec_layer(out)\n",
        "    out = self.predict(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "2yma_Vbc0k9n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(\n",
        "    vocab_size=vocab_size,\n",
        "    dmodel=dmodel,\n",
        "    dq=dq,\n",
        "    dk=dk,\n",
        "    dv=dv,\n",
        "    d_ff=d_ff,\n",
        "    heads=heads,\n",
        "    mask=None,\n",
        "    num_layers=1  # Single decoder layer for simplicity\n",
        ")\n"
      ],
      "metadata": {
        "id": "M62_C8zS06ya"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "n4y0aNcr2j1e"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_ids, labels, epochs=1000):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        out = model(input_ids)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(out.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Log progress every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "6uhXU0ae2rxU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model for 10K epochs\n",
        "train(x, y, epochs=10000)\n",
        "# torch.Size([10, 28, 32])"
      ],
      "metadata": {
        "id": "sHKeW38t2w4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97874325-4445-4568-fccc-ed4d99bfabd9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/10000], Loss: 7.6016\n",
            "Epoch [200/10000], Loss: 5.8381\n",
            "Epoch [300/10000], Loss: 5.2240\n",
            "Epoch [400/10000], Loss: 4.9821\n",
            "Epoch [500/10000], Loss: 4.8680\n",
            "Epoch [600/10000], Loss: 4.8026\n",
            "Epoch [700/10000], Loss: 4.7623\n",
            "Epoch [800/10000], Loss: 4.7297\n",
            "Epoch [900/10000], Loss: 4.7069\n",
            "Epoch [1000/10000], Loss: 4.6889\n",
            "Epoch [1100/10000], Loss: 4.6721\n",
            "Epoch [1200/10000], Loss: 4.6568\n",
            "Epoch [1300/10000], Loss: 4.6409\n",
            "Epoch [1400/10000], Loss: 4.6270\n",
            "Epoch [1500/10000], Loss: 4.6122\n",
            "Epoch [1600/10000], Loss: 4.5986\n",
            "Epoch [1700/10000], Loss: 4.5851\n",
            "Epoch [1800/10000], Loss: 4.5698\n",
            "Epoch [1900/10000], Loss: 4.5551\n",
            "Epoch [2000/10000], Loss: 4.5410\n",
            "Epoch [2100/10000], Loss: 4.5268\n",
            "Epoch [2200/10000], Loss: 4.5118\n",
            "Epoch [2300/10000], Loss: 4.4944\n",
            "Epoch [2400/10000], Loss: 4.4783\n",
            "Epoch [2500/10000], Loss: 4.4583\n",
            "Epoch [2600/10000], Loss: 4.4416\n",
            "Epoch [2700/10000], Loss: 4.4240\n",
            "Epoch [2800/10000], Loss: 4.4068\n",
            "Epoch [2900/10000], Loss: 4.3895\n",
            "Epoch [3000/10000], Loss: 4.3718\n",
            "Epoch [3100/10000], Loss: 4.3514\n",
            "Epoch [3200/10000], Loss: 4.3301\n",
            "Epoch [3300/10000], Loss: 4.3093\n",
            "Epoch [3400/10000], Loss: 4.2830\n",
            "Epoch [3500/10000], Loss: 4.2626\n",
            "Epoch [3600/10000], Loss: 4.2408\n",
            "Epoch [3700/10000], Loss: 4.2191\n",
            "Epoch [3800/10000], Loss: 4.1937\n",
            "Epoch [3900/10000], Loss: 4.1670\n",
            "Epoch [4000/10000], Loss: 4.1428\n",
            "Epoch [4100/10000], Loss: 4.1180\n",
            "Epoch [4200/10000], Loss: 4.0936\n",
            "Epoch [4300/10000], Loss: 4.0696\n",
            "Epoch [4400/10000], Loss: 4.0454\n",
            "Epoch [4500/10000], Loss: 4.0200\n",
            "Epoch [4600/10000], Loss: 3.9939\n",
            "Epoch [4700/10000], Loss: 3.9687\n",
            "Epoch [4800/10000], Loss: 3.9420\n",
            "Epoch [4900/10000], Loss: 3.9139\n",
            "Epoch [5000/10000], Loss: 3.8843\n",
            "Epoch [5100/10000], Loss: 3.8568\n",
            "Epoch [5200/10000], Loss: 3.8234\n",
            "Epoch [5300/10000], Loss: 3.7944\n",
            "Epoch [5400/10000], Loss: 3.7654\n",
            "Epoch [5500/10000], Loss: 3.7355\n",
            "Epoch [5600/10000], Loss: 3.7063\n",
            "Epoch [5700/10000], Loss: 3.6754\n",
            "Epoch [5800/10000], Loss: 3.6388\n",
            "Epoch [5900/10000], Loss: 3.6072\n",
            "Epoch [6000/10000], Loss: 3.5717\n",
            "Epoch [6100/10000], Loss: 3.5407\n",
            "Epoch [6200/10000], Loss: 3.5095\n",
            "Epoch [6300/10000], Loss: 3.4754\n",
            "Epoch [6400/10000], Loss: 3.4415\n",
            "Epoch [6500/10000], Loss: 3.4095\n",
            "Epoch [6600/10000], Loss: 3.3752\n",
            "Epoch [6700/10000], Loss: 3.3421\n",
            "Epoch [6800/10000], Loss: 3.3047\n",
            "Epoch [6900/10000], Loss: 3.2681\n",
            "Epoch [7000/10000], Loss: 3.2330\n",
            "Epoch [7100/10000], Loss: 3.1983\n",
            "Epoch [7200/10000], Loss: 3.1640\n",
            "Epoch [7300/10000], Loss: 3.1233\n",
            "Epoch [7400/10000], Loss: 3.0876\n",
            "Epoch [7500/10000], Loss: 3.0542\n",
            "Epoch [7600/10000], Loss: 3.0182\n",
            "Epoch [7700/10000], Loss: 2.9806\n",
            "Epoch [7800/10000], Loss: 2.9354\n",
            "Epoch [7900/10000], Loss: 2.9002\n",
            "Epoch [8000/10000], Loss: 2.8617\n",
            "Epoch [8100/10000], Loss: 2.8178\n",
            "Epoch [8200/10000], Loss: 2.7770\n",
            "Epoch [8300/10000], Loss: 2.7361\n",
            "Epoch [8400/10000], Loss: 2.6976\n",
            "Epoch [8500/10000], Loss: 2.6577\n",
            "Epoch [8600/10000], Loss: 2.6182\n",
            "Epoch [8700/10000], Loss: 2.5778\n",
            "Epoch [8800/10000], Loss: 2.5390\n",
            "Epoch [8900/10000], Loss: 2.4969\n",
            "Epoch [9000/10000], Loss: 2.4556\n",
            "Epoch [9100/10000], Loss: 2.4146\n",
            "Epoch [9200/10000], Loss: 2.3752\n",
            "Epoch [9300/10000], Loss: 2.3311\n",
            "Epoch [9400/10000], Loss: 2.2904\n",
            "Epoch [9500/10000], Loss: 2.2487\n",
            "Epoch [9600/10000], Loss: 2.2014\n",
            "Epoch [9700/10000], Loss: 2.1579\n",
            "Epoch [9800/10000], Loss: 2.1198\n",
            "Epoch [9900/10000], Loss: 2.0752\n",
            "Epoch [10000/10000], Loss: 2.0218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss is about 2.0218 after 10K epochs"
      ],
      "metadata": {
        "id": "J9Sti5BtU03d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate text**"
      ],
      "metadata": {
        "id": "RgLu0ijo-cdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate(model, prompt=['<start>'], max_words=10):\n",
        "    \"\"\"\n",
        "    Generate a sequence of words using a model given an initial prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The language model to use for generation.\n",
        "        prompt (list): List of initial tokens to start generation.\n",
        "        max_words (int): Maximum number of words to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated sequence as a string.\n",
        "    \"\"\"\n",
        "    # Tokenizer initialization (assuming `text` contains your data)\n",
        "    tokenizer = Tokenizer(text)\n",
        "    vocab = tokenizer.build_vocab()\n",
        "    stoi = vocab.vocab.get_stoi()\n",
        "    itos = vocab.vocab.get_itos()\n",
        "\n",
        "    # Validate the prompt\n",
        "    if isinstance(prompt, list):\n",
        "        for token in prompt:\n",
        "            if token not in stoi:\n",
        "                raise ValueError(f\"Token '{token}' not found in vocabulary.\")\n",
        "    elif prompt not in stoi:\n",
        "        raise ValueError(f\"Token '{prompt}' not found in vocabulary.\")\n",
        "\n",
        "    # Convert prompt tokens to input IDs\n",
        "    input_ids = torch.tensor([[stoi[token] for token in prompt]], dtype=torch.long)  # Shape: (1, len(prompt))\n",
        "\n",
        "    # Initialize the generated sequence\n",
        "    generated = input_ids.clone()\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        # Pass the sequence through the model\n",
        "        output = model(generated)  # Output shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        # Extract logits for the last token in the sequence\n",
        "        logits = output[:, -1, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "        # Determine the next token ID\n",
        "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # Shape: (1, 1)\n",
        "\n",
        "        # Append the predicted token ID to the sequence\n",
        "        generated = torch.cat((generated, next_token_id), dim=1)\n",
        "\n",
        "        # Stop generation if the <end> token is generated\n",
        "        if next_token_id.item() == stoi['<end>']:\n",
        "            break\n",
        "\n",
        "    # Convert generated token IDs back to words\n",
        "    generated_sequence = [itos[token_id] for token_id in generated.squeeze().tolist()]\n",
        "\n",
        "    return ' '.join(generated_sequence)\n"
      ],
      "metadata": {
        "id": "TYOkVHDV5TKf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Tk.vocab.vocab.get_stoi())  # Check stoi mappings\n",
        "print(Tk.vocab.vocab.get_itos()[:10])  # Check first 10 itos mappings\n"
      ],
      "metadata": {
        "id": "1JwTY_7unrms",
        "outputId": "44139413-defb-4af6-8927-6a4e310116fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'years': 154, 'which': 151, 'what': 150, 'videos': 147, 'unique': 143, 'turning': 141, 'timeless': 139, 'than': 134, 'succeeding': 133, 'stories': 131, 'skills': 130, 'containing': 51, 'problem-solving': 113, 'who': 30, 'research': 123, 'has': 76, 'best': 46, 'paid': 103, 'benefits': 45, 'been': 44, 'version': 146, 'assumptions': 40, 'attention': 43, 'us': 145, 'part': 104, 'attempts': 42, '<start>': 1, ',': 3, 'attempted': 41, 'project': 114, 'as': 39, 'was': 149, 'those': 137, 'applied': 37, 'an': 36, 'uncovered': 142, 'also': 35, 'drove': 62, 'chronicle': 49, 'opportunities': 101, 'correcting': 54, 'about': 32, 'qualities': 117, 'tribute': 29, 'continue': 53, 'this': 17, 'error': 64, 'that': 27, 'multi-player': 93, 'should': 128, 'codes': 50, 'with': 31, 'by': 48, 'a': 7, 'questions': 118, 'me': 16, 'writings': 153, 'i': 12, 'accomplishments': 33, '.': 5, 'individual': 81, '<end>': 2, 'demonstrates': 56, 'few': 68, 'wanted': 148, 'is': 13, 'true': 140, 'he': 21, 'decided': 55, 'the': 6, 'hamming': 11, 'world': 152, 'and': 4, 'articles': 38, 'for': 70, 'legacy': 14, \"'\": 18, 'polymath': 111, 'previous': 112, 'unpublished': 144, 'details': 57, 'open': 100, 'one': 99, 'personal': 107, 'their': 28, 'events': 20, 'they': 136, 'including': 22, 'achieved': 34, 'each': 63, 'inspired': 23, 'my': 94, 'life': 25, 'context': 52, 's': 26, 'known': 24, 'developed': 59, 'disciplines': 60, 'him': 79, 'discovered': 61, 'five': 69, 'to': 9, 'develop': 58, 'particular': 105, 'biography': 15, 'exceptional': 66, 'his': 10, 'facts': 67, 'from': 71, 'significant': 129, 'in': 80, 'shaped': 127, 'enabled': 19, 'generations': 72, 'mathematical': 90, 'of': 8, 'gift': 74, '<unk>': 0, 'places': 109, 'guidance': 75, 'helped': 77, 'more': 92, 'thought': 138, 'numerous': 97, 'helping': 78, 'influence': 82, 'influences': 83, 'information': 84, 'intertwines': 85, 'excellence': 65, 'invention': 86, 'story': 132, 'list': 88, 'many': 89, 'memoir': 91, 'not': 96, 'on': 98, 'leave': 87, 'noble': 95, 'others': 102, 'people': 106, 'photographs': 108, 'points': 110, 'put': 116, 'recalled': 119, 'book': 47, 'received': 120, 'genre': 73, 'reflecting': 121, 'remembrances': 122, 'there': 135, 'restate': 124, 'result': 125, 'pursuit': 115, 'reveal': 126}\n",
            "['<unk>', '<start>', '<end>', ',', 'and', '.', 'the', 'a', 'of', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, prompt=['<start>'], max_words=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xcA1Lu4j6Eq-",
        "outputId": "9b5c65e8-2010-4957-b4db-53671e0375c4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i , biography of <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note the model has memorized the sentence from the training set. Given the start token, if your implementation reproduce a sentence as is in the training set, then your implementation is likely to be correct.\n",
        "* Suppose the prompt is `<start> best known`, then we expect the model to produce the first sentence as is"
      ],
      "metadata": {
        "id": "sTaqoJ_2VDDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model,prompt=['<start>','best','known'],max_words=25)"
      ],
      "metadata": {
        "id": "2N_Shu7TXjAT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c2249d89-5662-48ab-8fda-2248361c5a53"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> best known hamming <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Change the prompt"
      ],
      "metadata": {
        "id": "P1hK47T5X2fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model,prompt=['<start>','reflecting','on'],max_words=25)"
      ],
      "metadata": {
        "id": "Q6_WM1K2X7qZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dfe23f66-ec9b-42c8-ca89-7784782757d2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> reflecting on develop significant to mathematical a the is the years world a open continue a demonstrates a was to hamming was was was to biography in'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model,prompt=['<start>','this','is'],max_words=25)"
      ],
      "metadata": {
        "id": "BsZXZf_OnBui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "44c38cde-52e6-4e01-81b0-b6d2f61afd12"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> this is a the a the a and pursuit a , is a who <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DhTxtQ0ZMpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}